QUESTION: ‘Are you having a nice day?’ ANSWER: ‘As an AI language model, I don’t have personal experiences or the ability to perceive the world, so I don’t have the concept of a “day” or the ability to have feelings such as “nice” or “not nice.” My purpose is to provide helpful and informative responses to assist with your questions and needs. Is there anything specific you would like to know or talk about?’ Forming a friendship with ChatGPT is no easy task. The Artificial Intelligence (AI) system, also known as a ‘Chatbot’, was made available for public testing in early December. You can ask for curated jokes or funny songs – and some people have even used it to pass exams. ChatGPT cheerily promises to ‘always bring a smile to your face’ during conversations. Exchanges don’t feel hugely dissimilar to talking with a real-life friend. Some developers have even suggested using AI to address the loneliness epidemic sweeping the world. But can Chat GPT really become our friend? And if it can, should we be worried? Şerife Tekin, an associate professor of philosophy at the University of Texas, reckons so. While it may seem easy venting to the unassuming Chat GDP, it can never fully replace a real life friendship – no matter how clever its AI system is. Şerife told Metro.co.uk: ‘It would be wrong to overstate its capacities for responding to human challenges such as curbing loneliness, or supporting mental health in general. ‘As evident in your conversation with it, ChatGPT itself acknowledges that it is only a language model, and that it does not have human-like experiences and agency. ‘Rather than substituting humans to serve as a friend or a connection, ChatGPT provides information about how one might address loneliness. ‘The answer to the question about curbing loneliness requires understanding what loneliness is.’ Loneliness comes from a lack of close friendships, Şerife says. And close, real-life, relationships aren’t simple. They need a healthy amount of pushback and criticism to allow each friend to grow, she explained. Şerife said: ‘To flourish as a human being we do need our friends to call on us out on our mistakes, challenge our behaviour so that we work towards a better version of ourselves.’ ‘Friendship is an activity, where individuals engage in a dynamic relationship of thinking together, exchanging reasons, beliefs, desires, fears, in a way that results with enhancing the agency and happiness of each party. ‘Yes, perhaps ChatGPT can spit advice to its users, or may feel like it is not judging what we share with them, but that is not real friendship. ‘I think we should absolutely worry about even a remote suggestion that ChatGPT can fix loneliness. It cannot.’ The system’s rise in popularity comes years after the invention of mental health ‘bots’. So-called online ‘therapists’ allow for people to vent to an anonymous system that will never really judge, not really. Mental health charities have criticised the approach over the years – saying virtual support will never compare to a human reaction. QUESTION: I am sad ANSWER: ‘I’m sorry that you’re feeling sad. It’s normal to experience a wide range of emotions, and it’s important to take care of yourself during these times. If you’re comfortable, it may be helpful to talk to a trusted friend, family member, or mental health professional about what’s been causing you to feel sad. ‘There are also many other things you can do to help improve your mood, such as engaging inactivity that you enjoy, spending time in nature, exercise, mindfulness or seeking support from online communities. ‘Remember, it’s okay to not be okay, and seeking support is a strength, not a weakness.’ Could AI systems, such as ChatGPT, ever truly understand the unpredictability and the intricacies of sadness? No, Şerife firmly explains. She also touches on the growing trend of referring to AI systems as ‘virtual therapists’, and the connotations it has. The academic explained: ‘It is incorrect to call what Chat_Bots do “therapy,” nor can we call Chat_Bots “therapists” – even though they are promoted and advertised this way by their developers. ‘In modern psychology, medicine, and psychiatry, therapy has been used to address something taking place between (at least) two agents: the healthcare professional and the patient. ‘Thus, calling Chat_Bots therapists implies calling them persons or agents, but they are far from that. If Chat_Bots are not agents or persons, they cannot be expected to be in charge of addressing mental health concerns of the individuals using them. ‘This poses an ethical problem: by implying users will receive therapy from an agent, a Chat_Bots gives a false promise and thereby overstates its potential benefit.’ Debate is rife between those creating AI systems and those tentatively questioning its usefulness. So, we put the question of friendship to ChatGPT for a final say. QUESTION: Do you think you can be someone’s friend? ANSWER: As an AI language model, I am not capable of forming personal relationships or empathy like a human friend would. I can provide information and answer questions, but I am not capable of providing emotional support or forming a personal bond with users. My purpose is to assist with tasks and answer questions to the best of my ability based on the information I have been trained to do. QUESTION: Do you think you help people feel less lonely? ANSWER: As an AI language model, I don’t have personal opinions or beliefs. However, some people may find chatting with me to be a helpful distraction from loneliness or to provide comfort and a sense of companionship. Others my use me to obtain information or help with tasks. ‘Ultimately, the impact I have on an individual’s feelings of loneliness will vary depending on their individual circumstances and the way they choose to use me.’ Get in touch with our news team by emailing us at webnews@metro.co.uk. For more stories like this, check our news page.